{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BentoML Example: H2O Classification\n",
    "\n",
    "\n",
    "[BentoML](http://bentoml.ai) is an open source platform for machine learning model serving and deployment. \n",
    "\n",
    "This notebook demonstrates how to use BentoML to __turn a H2O model into a docker image containing a REST API server__ serving this model, as well as distributing your model as a command line tool or a pip-installable PyPI package.\n",
    "\n",
    "![Impression](https://www.google-analytics.com/collect?v=1&tid=UA-112879361-3&cid=555&t=event&ec=nb&ea=open&el=official-example&dt=h2o-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install bentoml\n",
    "!pip install h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "import bentoml\n",
    "\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This show case considers prostate cancer data and tries to find an algorithm to prognose a certain phase of cancer. The dataset was collected at the Ohio State University Comprehensive Cancer Center and includes demographic and medical data from each of the 380 patients as well as a classifier identifying if the patients tumor has already penetrated the prostatic capsule. This latter event is a clear sign for an advanced cancer state and also helps the doctor to decide on biopsy and treatment methods.\n",
    "\n",
    "In this show case a deep learning algorithm is used to classify the tumors of the patients into 'penetrating prostatic capsule' and 'not penetrating prostatic capsule'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset & Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prostate = h2o.import_file(path=\"https://raw.githubusercontent.com/multicode/h2o-notebook/master/prostate.csv\")\n",
    "prostate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the deep learning estimator module\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "# transform the target variable into a factor\n",
    "prostate[\"CAPSULE\"] = prostate[\"CAPSULE\"].asfactor()\n",
    "# construct and define the estimator object \n",
    "model = H2ODeepLearningEstimator(activation = \"Tanh\", hidden = [10, 10, 10], epochs = 100)\n",
    "# train the model on the whole prostate dataset\n",
    "model.train(x = list(set(prostate.columns) - set([\"ID\",\"CAPSULE\"])), y =\"CAPSULE\", training_frame = prostate)\n",
    "model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(prostate)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define BentoService for model serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile h2o_model_service.py\n",
    "import pandas as pd\n",
    "import h2o\n",
    "import bentoml\n",
    "from bentoml.artifact import H2oModelArtifact\n",
    "from bentoml.handlers import DataframeHandler\n",
    "\n",
    "@bentoml.artifacts([H2oModelArtifact('model')])\n",
    "@bentoml.env(pip_dependencies=['h2o'])\n",
    "class H2oModelService(bentoml.BentoService):\n",
    "\n",
    "    @bentoml.api(DataframeHandler)\n",
    "    def predict(self, df):     \n",
    "        hf = h2o.H2OFrame(df)\n",
    "        predictions = self.artifacts.model.predict(hf)\n",
    "        return predictions.as_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save BentoService to file archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) import the custom BentoService defined above\n",
    "from h2o_model_service import H2oModelService\n",
    "\n",
    "# 2) `pack` it with required artifacts\n",
    "bento_svc = H2oModelService.pack(model=model)\n",
    "\n",
    "# 3) save your BentoSerivce\n",
    "saved_path = bento_svc.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BentoService from archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bentoml\n",
    "import pandas as pd\n",
    "\n",
    "# Load saved BentoService archive from file directory\n",
    "loaded_bento_svc = bentoml.load(saved_path)\n",
    "\n",
    "# Access the predict function of loaded BentoService\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/multicode/h2o-notebook/master/prostate.csv\")\n",
    "loaded_bento_svc.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"pip install\" a BentoService archive\n",
    "\n",
    "BentoML user can directly pip install saved BentoML archive with `pip install $SAVED_PATH`,  and use it as a regular python package.\n",
    "\n",
    "*For demo purpurse, copy generated model to ./model folder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install {saved_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your bentoML model class name will become packaged name\n",
    "import H2oModelService\n",
    "\n",
    "ms = H2oModelService.load() # call load to ensure all artifacts are loaded\n",
    "ms.predict(pd.read_csv('https://raw.githubusercontent.com/multicode/h2o-notebook/master/prostate.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use installed BentoService as CLI tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!H2oModelService --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!H2oModelService info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!H2oModelService predict --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!H2oModelService predict --input https://raw.githubusercontent.com/multicode/h2o-notebook/master/prostate.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serving via REST API\n",
    "\n",
    "In your termnial, run the following command to start the REST API server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml serve {saved_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send prediction request to REST API server\n",
    "\n",
    "Run the following command in terminal to make a HTTP request to the API server:\n",
    "```bash\n",
    "curl -i \\\n",
    "--header \"Content-Type: text/csv\" \\\n",
    "--request POST \\\n",
    "--data 'ID,CAPSULE,AGE,RACE,DPROS,DCAPS,PSA,VOL,GLEASON\\n\n",
    "1,0,65,1,2,1,1.4,0,6\\n\n",
    "2,0,72,1,3,2,6.7,0,7\\n' \\\n",
    "localhost:5000/predict\n",
    "```\n",
    "\n",
    "\n",
    "## Containerize REST API server with Docker\n",
    "\n",
    "** _Note: `docker` is not available when running in Google Colaboratory_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd \"./model\" && docker build -t h2o-model ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the server with docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -p 5000:5000 h2o-model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
